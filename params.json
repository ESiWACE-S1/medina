{
  "name": "Medina",
  "tagline": "MECCA development in accelerators - KPP Fortran to CUDA source-to-source pre-processor",
  "body": "# MEDINA: KPP Fortran to CUDA source-to-source pre-processor\r\n\r\n*Disclaimer: This software is in alpha-test mode, \r\nequivalent to the MESSy red traffic light status.\r\nNo unexpected behaviour was observed under testing, and users are \r\ninvited to test with their model setup. However, no express guarantee\r\nis provided for production simulations.* \r\n\r\n## 1. Requirements:\r\n\r\n###Software: \r\nCUDA compiler and python are required for the processor;\r\n[EMAC](http://www.messy-interface.org/) (ECHAM/MESSy) Earth System Model.\r\n         \r\n###Hardware: \r\nCUDA compatible GPU. \r\n\r\n## 2. Installation:\r\n\r\nThere are two files required to enable using the GPUs: \r\n`f2c\\_alpha.py`  and `kpp\\_integrate\\_cuda\\_prototype.cu`. \r\n\r\nThe files have to be available in the messy/util directory. \r\nNo additional changes are required. \r\n\r\nNote: MESSy has to be linked with the `-lcudart` flag. \r\nFor example, you can append it to the `SPEC_NETCDF_LIB` variable \r\nin the configuration file (under `config/mh-XXXX`).\r\n\r\n## 3. Running the MECCA Fortran to CUDA source-to-source pre-processor:\r\n\r\nYou have to enter the `./messy/util directory` to execute the\r\npreprocessor, by running \"`python f2c_alpha.py`\". The preprocessor expects\r\nthe following files to be in place:\r\n\r\n*     messy/smcl/messy_mecca_kpp.f90\r\n*     messy/smcl/messy_cmn_photol_mem.f90\r\n*     messy/smcl/messy_main_constants_mem.f90\r\n*     messy/util/kpp_integrate_cuda_prototype.cu\r\n*     messy/smcl/specific.mk\r\n*     messy/smcl/Makefile.m\r\n \r\n\r\nIf any of these files is missing or not configured as in the MESSy release,\r\nthe preprocessor will stop with an error message.\r\n\r\n## 4. Running EMAC with GPU MECCA and improving performance:\r\n\r\nThe runtime parameter `NPROMA` should be set to a value not greater than 128.\r\nThis allows for optimal memory allocation and performance on the GPU.\r\n\r\nEach CPU process that offloads to GPU requires a chunk of the GPU VRAM memory,\r\ndependent on the number of species and reaction constants in the MECCA mechanism. \r\nThe number of GPUs per node and VRAM memory available in each GPU dictates the\r\ntotal number of CPU cores that can run simultaneously.\r\n\r\nWarning: When running multiple CPU processes per GPU, if\r\nmemory is not enough the CUDA runtime will fail silently - without any\r\nerror. A solution in that case is to use the Multi-process service provided\r\nby NVIDIA as an alternative.\r\n\r\nDuring experiments with an engineering sample of the next generation \r\nNVIDIA Pascal architecture, the source application will fail due to \r\nlarge local memory requirements. Transforming runtime GPU local access to global \r\nsolves the problem, at a performance cost.\r\n\r\n### Authors and Contributors\r\nFor assistance or to report problems please contact the maintainers:\r\nT. Christoudias (@theoc); M. Alvanos (@malvanos)\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}